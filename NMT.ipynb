{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq Neural Machine Translation Model\n",
    "\n",
    "A tensorflow version of a basic seq2seq model for machine transklation with the NLP preprocessing.\n",
    "\n",
    "tesorflow-gpu=1.12.0\n",
    "\n",
    "GPU: 1080"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing & data loading\n",
    "\n",
    "Here, we use the NLP preprocess tricks taught in class to clean the input data for better training. First, we load the data as a really long string to later create a lookup table for embedding. Then, we split the data into sentences and pass them to the preprocessing function and then concate together to get the clean data for both source(English) and target(French) language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os, copy, pickle, re\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "\n",
    "    # Remove emotion signature and Capitalization\n",
    "    emotions = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)  \n",
    "    text = re.sub('[\\W|\\.]+', ' ', text.lower())+' '.join(emotions).replace('-', '')\n",
    "        \n",
    "    return text\n",
    "\n",
    "def load_data(source_path, target_path):\n",
    "    \n",
    "    source_input_file = os.path.join(source_path)\n",
    "    source_clean_data = []\n",
    "    \n",
    "    target_input_file = os.path.join(target_path)\n",
    "    target_clean_data = []\n",
    "    \n",
    "    with open(source_input_file, 'r', encoding='utf-8') as f:\n",
    "        source_data = f.read()\n",
    "    \n",
    "    with open(target_input_file, 'r', encoding='utf-8') as f:\n",
    "        target_data = f.read()\n",
    "    \n",
    "    source_sentences = source_data.split(\"\\n\")\n",
    "    target_sentences = target_data.split(\"\\n\")\n",
    "\n",
    "    for i in range(len(source_sentences)):\n",
    "        source_clean_data.append(preprocess(source_sentences[i]) + '\\n')\n",
    "        target_clean_data.append(preprocess(target_sentences[i]) + '\\n')\n",
    "\n",
    "    source_clean_data = \" \".join(source_clean_data)\n",
    "    target_clean_data = \" \".join(target_clean_data)\n",
    "\n",
    "    return source_clean_data, target_clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create lookup table\n",
    "\n",
    "For general NLP task, we not only need a lookup table for mapping vocabulary into numbers for training and inference but also need it to mapping the model output back into required vacabulary to check the model performance and application. Here, we create a lookup table for both cleanup source and target corpus with all the vacabulary in he corpus and also the special token for modle to recongnize the *padding*, *end of sentence*, *unknown*(out of vacabulary), *go*(for model start encoding and decoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_tables(text):\n",
    "    CODES = {'<PAD>': 0, '<EOS>': 1, '<UNK>': 2, '<GO>': 3 }\n",
    "\n",
    "    vocab = set(text.split())\n",
    "    vocab_to_int = copy.copy(CODES)\n",
    "\n",
    "    for v_i, v in enumerate(vocab, len(CODES)):\n",
    "        vocab_to_int[v] = v_i\n",
    "\n",
    "    int_to_vocab = {v_i: v for v, v_i in vocab_to_int.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapping the corpus into numbers\n",
    "\n",
    "with the lookup table, here we map the sentence into integers for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int):\n",
    "\n",
    "    source_text_id = []\n",
    "    target_text_id = []\n",
    "    \n",
    "    # make a list of sentences (extraction)\n",
    "    source_sentences = source_text.split(\"\\n\")\n",
    "    target_sentences = target_text.split(\"\\n\")\n",
    "        \n",
    "    max_source_sentence_length = max([len(sentence.split(\" \")) for sentence in source_sentences])\n",
    "    max_target_sentence_length = max([len(sentence.split(\" \")) for sentence in target_sentences])\n",
    "        \n",
    "    # iterating through each sentences (# of sentences in source&target is the same)\n",
    "    for i in range(len(source_sentences)):\n",
    "        # extract sentences one by one\n",
    "        source_sentence = source_sentences[i]\n",
    "        target_sentence = target_sentences[i]\n",
    "        \n",
    "        # make a list of tokens/words (extraction) from the chosen sentence\n",
    "        source_tokens = source_sentence.split(\" \")\n",
    "        target_tokens = target_sentence.split(\" \")\n",
    "        \n",
    "        # empty list of converted words to index in the chosen sentence\n",
    "        source_token_id = []\n",
    "        target_token_id = []\n",
    "        \n",
    "        for index, token in enumerate(source_tokens):\n",
    "            if (token != \"\"):\n",
    "                source_token_id.append(source_vocab_to_int[token])\n",
    "        \n",
    "        for index, token in enumerate(target_tokens):\n",
    "            if (token != \"\"):\n",
    "                target_token_id.append(target_vocab_to_int[token])\n",
    "                \n",
    "        # put <EOS> token at the end of the chosen target sentence\n",
    "        # this token suggests when to stop creating a sequence\n",
    "        target_token_id.append(target_vocab_to_int['<EOS>'])\n",
    "            \n",
    "        # add each converted sentences in the final list\n",
    "        source_text_id.append(source_token_id)\n",
    "        target_text_id.append(target_token_id)\n",
    "    \n",
    "    return source_text_id, target_text_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Tensorflow graph input\n",
    "\n",
    "Here we build the tensorflow graph input for encoder, decoder and hyper parameters (learning rate & drop out rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_dec_model_inputs():\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets') \n",
    "    \n",
    "    target_sequence_length = tf.placeholder(tf.int32, [None], name='target_sequence_length')\n",
    "    max_target_len = tf.reduce_max(target_sequence_length)    \n",
    "    \n",
    "    return inputs, targets, target_sequence_length, max_target_len\n",
    "\n",
    "def hyperparam_inputs():\n",
    "    lr_rate = tf.placeholder(tf.float32, name='lr_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return lr_rate, keep_prob\n",
    "\n",
    "def process_decoder_input(target_data, target_vocab_to_int, batch_size):\n",
    "    # get '<GO>' id\n",
    "    go_id = target_vocab_to_int['<GO>']\n",
    "    \n",
    "    after_slice = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    after_concat = tf.concat( [tf.fill([batch_size, 1], go_id), after_slice], 1)\n",
    "    \n",
    "    return after_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "We first create a embedding layer for extracting high level representation of the input sequence and then feed them into LSTM cell encoder layer for encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, source_vocab_size, encoding_embedding_size):\n",
    "\n",
    "    embed = tf.contrib.layers.embed_sequence(rnn_inputs, vocab_size=source_vocab_size, embed_dim=encoding_embedding_size)\n",
    "    \n",
    "    stacked_cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(rnn_size), keep_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    outputs, state = tf.nn.dynamic_rnn(stacked_cells, embed, dtype=tf.float32)\n",
    "\n",
    "    return outputs, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "In seq2seq model, the decoder is really special because the data flow is totally different between training and inference. For training the decoder also input the traget for learning but in the inference time, there are no anwsers so the input of decoder will be the previous decoder output and thus we need to define two model data flow for different scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, target_sequence_length, max_summary_length, output_layer, keep_prob):\n",
    "\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # for only input layer\n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(dec_embed_input, target_sequence_length)\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, helper, encoder_state, output_layer)\n",
    "\n",
    "    # unrolling the decoder layer\n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, impute_finished=True, maximum_iterations=max_summary_length)\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, end_of_sequence_id, max_target_sequence_length, vocab_size, output_layer, batch_size, keep_prob):\n",
    "\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob=keep_prob)\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings, tf.fill([batch_size], start_of_sequence_id), end_of_sequence_id)\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, helper, encoder_state, output_layer)\n",
    "    \n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, impute_finished=True, maximum_iterations=max_target_sequence_length)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "def decoding_layer(dec_input, encoder_state, target_sequence_length, max_target_sequence_length, rnn_size, num_layers, target_vocab_to_int, target_vocab_size,\n",
    "                   batch_size, keep_prob, decoding_embedding_size):\n",
    "\n",
    "    target_vocab_size = len(target_vocab_to_int)\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "    \n",
    "    cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(rnn_size) for _ in range(num_layers)])\n",
    "    \n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        output_layer = tf.layers.Dense(target_vocab_size)\n",
    "        train_output = decoding_layer_train(encoder_state, cells, dec_embed_input, target_sequence_length, max_target_sequence_length, output_layer, keep_prob)\n",
    "\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        infer_output = decoding_layer_infer(encoder_state, cells, dec_embeddings, target_vocab_to_int['<GO>'], target_vocab_to_int['<EOS>'], \n",
    "        \t\t\t\t\t\t\t\t\tmax_target_sequence_length, target_vocab_size, output_layer,batch_size,keep_prob)\n",
    "\n",
    "    return (train_output, infer_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build whole seq2swq model\n",
    "\n",
    "After building the encoder and decoder, here we construct the whole model for whole data flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size, target_sequence_length, max_target_sentence_length, source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size, rnn_size, num_layers, target_vocab_to_int):\n",
    "\n",
    "    enc_outputs, enc_states = encoding_layer(input_data, rnn_size, num_layers, keep_prob, source_vocab_size, enc_embedding_size)\n",
    "    \n",
    "    dec_input = process_decoder_input(target_data, target_vocab_to_int, batch_size)\n",
    "    \n",
    "    train_output, infer_output = decoding_layer(dec_input, enc_states, target_sequence_length, max_target_sentence_length, rnn_size, num_layers,\n",
    "                                              target_vocab_to_int, target_vocab_size, batch_size, keep_prob, dec_embedding_size)\n",
    "    \n",
    "    return train_output, infer_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utility function\n",
    "\n",
    "One limitation of the seq2seq model is that it requires the same sequence length for training and inference, so we build the padding function for each batch data. Also we define the get the batch of data, caculating accuracy, save and load parameters and so on function for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    \n",
    "    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n",
    "\n",
    "def get_batches(sources, targets, batch_size, source_pad_int, target_pad_int):\n",
    "    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(sources)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "\n",
    "        # Slice the right amount for the batch\n",
    "        sources_batch = sources[start_i:start_i + batch_size]\n",
    "        targets_batch = targets[start_i:start_i + batch_size]\n",
    "\n",
    "        # Pad\n",
    "        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n",
    "        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n",
    "\n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_targets_lengths = []\n",
    "        for target in pad_targets_batch:\n",
    "            pad_targets_lengths.append(len(target))\n",
    "\n",
    "        pad_source_lengths = []\n",
    "        for source in pad_sources_batch:\n",
    "            pad_source_lengths.append(len(source))\n",
    "\n",
    "        yield pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths\n",
    "\n",
    "def get_accuracy(target, logits):\n",
    "    \"\"\"\n",
    "    Calculate accuracy\n",
    "    \"\"\"\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1])],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target, logits))\n",
    "\n",
    "def save_params(params):\n",
    "    with open('params.p', 'wb') as out_file:\n",
    "        pickle.dump(params, out_file)\n",
    "\n",
    "def load_params():\n",
    "    with open('params.p', mode='rb') as in_file:\n",
    "        return pickle.load(in_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters\n",
    "\n",
    "After various tuning, we choose batch size = 128, learning rate = 1e-3 and drop out rate = 0.5 to avoid overfitting. Also, for model capacity, we decide to use size = 128 cell and 3 layers for LSTM layer. At the same time, to better capture input data relation, we set embedding size = 200 for extracting more high level information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### hyperparameters ###\n",
    "display_step = 300\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "rnn_size = 128\n",
    "num_layers = 3\n",
    "\n",
    "encoding_embedding_size = 200\n",
    "decoding_embedding_size = 200\n",
    "\n",
    "learning_rate = 1e-3\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "The workflow is loading data -> create lookup table -> text to integer -> padding -> encoder & decoder training -> save final model for inference and application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch  300/1076 - Train Accuracy: 0.4353, Validation Accuracy: 0.4655, Loss: 1.9718\n",
      "Epoch   0 Batch  600/1076 - Train Accuracy: 0.5078, Validation Accuracy: 0.5238, Loss: 1.2119\n",
      "Epoch   0 Batch  900/1076 - Train Accuracy: 0.5432, Validation Accuracy: 0.5366, Loss: 0.9591\n",
      "Epoch   1 Batch  300/1076 - Train Accuracy: 0.6020, Validation Accuracy: 0.5798, Loss: 0.7234\n",
      "Epoch   1 Batch  600/1076 - Train Accuracy: 0.6287, Validation Accuracy: 0.6098, Loss: 0.6413\n",
      "Epoch   1 Batch  900/1076 - Train Accuracy: 0.6933, Validation Accuracy: 0.6225, Loss: 0.5664\n",
      "Epoch   2 Batch  300/1076 - Train Accuracy: 0.7352, Validation Accuracy: 0.6768, Loss: 0.4580\n",
      "Epoch   2 Batch  600/1076 - Train Accuracy: 0.7660, Validation Accuracy: 0.7730, Loss: 0.3965\n",
      "Epoch   2 Batch  900/1076 - Train Accuracy: 0.7985, Validation Accuracy: 0.7924, Loss: 0.3842\n",
      "Epoch   3 Batch  300/1076 - Train Accuracy: 0.8299, Validation Accuracy: 0.8121, Loss: 0.2760\n",
      "Epoch   3 Batch  600/1076 - Train Accuracy: 0.8372, Validation Accuracy: 0.8339, Loss: 0.2531\n",
      "Epoch   3 Batch  900/1076 - Train Accuracy: 0.8701, Validation Accuracy: 0.8643, Loss: 0.2589\n",
      "Epoch   4 Batch  300/1076 - Train Accuracy: 0.8776, Validation Accuracy: 0.8853, Loss: 0.1760\n",
      "Epoch   4 Batch  600/1076 - Train Accuracy: 0.8877, Validation Accuracy: 0.8997, Loss: 0.1681\n",
      "Epoch   4 Batch  900/1076 - Train Accuracy: 0.9025, Validation Accuracy: 0.9260, Loss: 0.1731\n",
      "Epoch   5 Batch  300/1076 - Train Accuracy: 0.9167, Validation Accuracy: 0.9371, Loss: 0.0930\n",
      "Epoch   5 Batch  600/1076 - Train Accuracy: 0.9046, Validation Accuracy: 0.9313, Loss: 0.1231\n",
      "Epoch   5 Batch  900/1076 - Train Accuracy: 0.9264, Validation Accuracy: 0.9350, Loss: 0.1241\n",
      "Epoch   6 Batch  300/1076 - Train Accuracy: 0.9562, Validation Accuracy: 0.9412, Loss: 0.0684\n",
      "Epoch   6 Batch  600/1076 - Train Accuracy: 0.9280, Validation Accuracy: 0.9490, Loss: 0.0932\n",
      "Epoch   6 Batch  900/1076 - Train Accuracy: 0.9309, Validation Accuracy: 0.9519, Loss: 0.1011\n",
      "Epoch   7 Batch  300/1076 - Train Accuracy: 0.9488, Validation Accuracy: 0.9523, Loss: 0.0603\n",
      "Epoch   7 Batch  600/1076 - Train Accuracy: 0.9478, Validation Accuracy: 0.9515, Loss: 0.0685\n",
      "Epoch   7 Batch  900/1076 - Train Accuracy: 0.9322, Validation Accuracy: 0.9544, Loss: 0.0813\n",
      "Epoch   8 Batch  300/1076 - Train Accuracy: 0.9453, Validation Accuracy: 0.9445, Loss: 0.0478\n",
      "Epoch   8 Batch  600/1076 - Train Accuracy: 0.9297, Validation Accuracy: 0.9609, Loss: 0.0637\n",
      "Epoch   8 Batch  900/1076 - Train Accuracy: 0.9404, Validation Accuracy: 0.9576, Loss: 0.0767\n",
      "Epoch   9 Batch  300/1076 - Train Accuracy: 0.9575, Validation Accuracy: 0.9408, Loss: 0.0399\n",
      "Epoch   9 Batch  600/1076 - Train Accuracy: 0.9437, Validation Accuracy: 0.9544, Loss: 0.0581\n",
      "Epoch   9 Batch  900/1076 - Train Accuracy: 0.9420, Validation Accuracy: 0.9622, Loss: 0.0732\n",
      "Model Trained and Saved\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### data loading ###    \n",
    "source_text, target_text = load_data('en.txt', 'fr.txt')\n",
    "\n",
    "# create lookup tables\n",
    "source_vocab_to_int, source_int_to_vocab = create_lookup_tables(source_text)\n",
    "target_vocab_to_int, target_int_to_vocab = create_lookup_tables(target_text)\n",
    "\n",
    "# create list of sentences whose words are represented in index\n",
    "source_int_text, target_int_text = text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int)\n",
    "\n",
    "max_target_sentence_length = max([len(sentence) for sentence in source_int_text])\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    input_data, targets, target_sequence_length, max_target_sequence_length = enc_dec_model_inputs()\n",
    "    lr, keep_prob = hyperparam_inputs()\n",
    "    \n",
    "    train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]), targets, keep_prob, batch_size, target_sequence_length, max_target_sequence_length,\n",
    "                                                   len(source_vocab_to_int), len(target_vocab_to_int), encoding_embedding_size, decoding_embedding_size,\n",
    "                                                   rnn_size, num_layers, target_vocab_to_int)\n",
    "    \n",
    "    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "\n",
    "    # - Returns a mask tensor representing the first N positions of each cell.\n",
    "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function - weighted softmax cross entropy\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(training_logits, targets, masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "\n",
    "\n",
    "# Split data to training and validation sets\n",
    "train_source = source_int_text[batch_size:]\n",
    "train_target = target_int_text[batch_size:]\n",
    "valid_source = source_int_text[:batch_size]\n",
    "valid_target = target_int_text[:batch_size]\n",
    "\n",
    "valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths = next(get_batches(valid_source, valid_target, batch_size, source_vocab_to_int['<PAD>'], target_vocab_to_int['<PAD>']))                                                                                                  \n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    tac = []\n",
    "    vac = []\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n",
    "                get_batches(train_source, train_target, batch_size,\n",
    "                            source_vocab_to_int['<PAD>'],\n",
    "                            target_vocab_to_int['<PAD>'])):\n",
    "\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: source_batch,\n",
    "                 targets: target_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: targets_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                batch_train_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: source_batch,\n",
    "                     target_sequence_length: targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "                batch_valid_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: valid_sources_batch,\n",
    "                     target_sequence_length: valid_targets_lengths,\n",
    "                     keep_prob: 1.0})\n",
    "\n",
    "                train_acc = get_accuracy(target_batch, batch_train_logits)\n",
    "                valid_acc = get_accuracy(valid_targets_batch, batch_valid_logits)\n",
    "\n",
    "                print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.4f}, Validation Accuracy: {:>6.4f}, Loss: {:>6.4f}'\n",
    "                      .format(epoch_i, batch_i, len(source_int_text) // batch_size, train_acc, valid_acc, loss))\n",
    "        \n",
    "        tac.append(train_acc)\n",
    "        vac.append(valid_acc)\n",
    "\n",
    "\t# Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, 'checkpoints/dev')\n",
    "    print('Model Trained and Saved')\n",
    "    save_params('checkpoints/dev')\n",
    "    \n",
    "### plot learning curve ###\n",
    "plt.plot(tac, '--', color=\"#111111\",  label=\"Training accuracy\")\n",
    "plt.plot(vac, color=\"#111111\", label=\"Validation accuracy\")\n",
    "\n",
    "# Create plot\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"epochs\"), plt.ylabel(\"Accuracy\"), plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('learning_curve.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "After finish training the model, here we test our model ability for inferencing several test sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English to be translated:\n",
      "the united states is sometimes mild during june , and it is cold in september .\n",
      "\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/dev\n",
      "\n",
      "Traget Sentence:\n",
      "les tats unis est parfois doux en juin et il est froid en septembre <EOS>\n"
     ]
    }
   ],
   "source": [
    "def sentence_to_seq(sentence, vocab_to_int):\n",
    "    results = []\n",
    "    for word in sentence.split(\" \"):\n",
    "        if word in vocab_to_int:\n",
    "            results.append(vocab_to_int[word])\n",
    "        else:\n",
    "            results.append(vocab_to_int['<UNK>'])\n",
    "            \n",
    "    return results\n",
    "   \n",
    "load_path = load_params()\n",
    "loaded_graph = tf.Graph()\n",
    "\n",
    "source_text, target_text = load_data('en.txt', 'fr.txt')\n",
    "\n",
    "source_vocab_to_int, source_int_to_vocab = create_lookup_tables(source_text)\n",
    "target_vocab_to_int, target_int_to_vocab = create_lookup_tables(target_text)\n",
    "\n",
    "input_sentence = input('English to be translated:\\n')\n",
    "print('\\n')\n",
    "\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "    loader.restore(sess, load_path)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    \n",
    "    translate_sentence = sentence_to_seq(input_sentence, source_vocab_to_int)\n",
    "\n",
    "    translate_logits = sess.run(logits, {input_data: [translate_sentence]*batch_size, target_sequence_length: [len(translate_sentence)*2]*batch_size, keep_prob: 1.0})[0]\n",
    "\n",
    "print(\"\\nTraget Sentence:\")\n",
    "print('{}'.format(\" \".join([target_int_to_vocab[i] for i in translate_logits])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
